# ğŸ“© Day 2: Spam Detection Model using NaÃ¯ve Bayes  

---

## ğŸ“Œ Overview  
In this project, we build a **Spam Detection Model** using the **NaÃ¯ve Bayes** algorithm. The goal is to classify SMS/email messages as **Spam or Not Spam** (Ham) based on text features. We utilize **Natural Language Processing (NLP) techniques** to clean and preprocess text data before training the model.  

---

## ğŸš€ What You'll Learn  
âœ… Text Preprocessing (Tokenization, Lemmatization, Stopword Removal)  
âœ… Feature Extraction (TF-IDF, Count Vectorization)  
âœ… Building a **NaÃ¯ve Bayes Classifier** for Text Classification  
âœ… Model Evaluation using Precision, Recall, F1-score & Confusion Matrix  
âœ… Handling Imbalanced Datasets  

---

## ğŸ“‚ Project Structure  
```
Day-2-Spam-Detection/
â”‚â”€â”€ README.md  # Documentation  
â”‚â”€â”€ spam_detection.ipynb  # Interactive Notebook  
â”‚â”€â”€ spam_detection.py  # Standalone Python Script  
â”‚â”€â”€ spam_dataset.csv  # Dataset  
```
---

##ğŸ“Œ Why Both .py and .ipynb?  
This project includes:  

ğŸ“’ **Jupyter Notebook (.ipynb)** â†’ For interactive exploration, visualization, and debugging.  
ğŸ’» **Python Script (.py)** â†’ For direct execution and efficiency.  

---

##ğŸ“Š Dataset  
For this project, we use a dataset containing:  

ğŸ“Œ **Features:** SMS/Email text messages  
ğŸ¯ **Target Variable:** Label (Spam or Not Spam)  

---

##ğŸ”§ Technologies Used  
ğŸ”¹ Python  
ğŸ”¹ Pandas & NumPy (Data Processing)  
ğŸ”¹ Scikit-learn (Model Building & Evaluation)  
ğŸ”¹ NLTK & spaCy (Text Preprocessing)  
ğŸ”¹ Matplotlib & Seaborn (Visualization)  

---

##ğŸ“œ How to Run the Project?  
1ï¸âƒ£ Clone the repository  
```bash
git clone https://github.com/Aadhityan-Senthil/30-Day-Applied-AI-Challenge.git  
cd 30-Day-Applied-AI-Challenge/Day-2-Spam-Detection with Navie Bayes
```
2ï¸âƒ£ Install dependencies  
```bash
pip install -r requirements.txt  
```
3ï¸âƒ£ Run the script  
```bash
python spam_detection.py
```
---

##ğŸ“ˆ Results & Analysis  
âœ… **Best Model Performance:**  
ğŸ“Œ **Accuracy:** 96.5%  
ğŸ“Œ **Precision:** 94.8%  
ğŸ“Œ **Recall:** 91.2%  
ğŸ“Œ **F1-score:** 93.0%  

âœ… **Observations:**  
ğŸ”¹ **TF-IDF Vectorization** improved model performance over Count Vectorization.  
ğŸ”¹ **NaÃ¯ve Bayes** is effective for text classification due to its probabilistic nature.  
ğŸ”¹ **Handling stopwords and lemmatization** resulted in better feature extraction.  
ğŸ”¹ **Class imbalance handling (using SMOTE/undersampling)** improved recall for spam detection.  

---

##ğŸ“Œ Next Steps  
ğŸ”¹ Experiment with **different feature extraction techniques** like word embeddings.  
ğŸ”¹ Try **other classifiers** (Logistic Regression, SVM) for comparison.  
ğŸ”¹ Fine-tune **hyperparameters** to optimize model performance.  

---

##â­ Contribute & Connect!  
ğŸ“¢ Follow my **30-day AI challenge** & share your feedback! ğŸš€ğŸ”¥  
